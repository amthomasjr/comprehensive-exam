\documentclass[10pt, compress, notheorems, aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm, amsmath, mathrsfs, amsfonts, bm, mathtools}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage{breqn}
\usepackage[backend=biber]{biblatex}
	\addbibresource{ref.bib}
\usepackage{tabulary}
\usepackage{multirow}
\usepackage{filecontents}
\usepackage{comment}

\usefonttheme{serif}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose \bgroup \originalleft}
\renewcommand{\right}{\aftergroup \egroup \originalright}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank} 

% \hypersetup{colorlinks=true} % Enable colored hyperlinks

\definecolor{utablue}{RGB}{0,100,177}% Official RGB code for UTA blue 
\definecolor{utaorange}{RGB}{245,128,38} 
\definecolor{utablue2}{RGB}{0,68,124}
\definecolor{utablue3}{RGB}{212,239,252}
\definecolor{utablue4}{RGB}{231,246,253}%R-231 G-246 B-253

\setbeamercolor{palette primary}{bg=utablue2,fg=utablue2}
\setbeamercolor{palette secondary}{bg=utablue2,fg=white} %footer color
\setbeamercolor{palette tertiary}{bg=black,fg=utablue2}% header color
\setbeamercolor{palette quaternary}{bg=utablue2,fg=black}
\setbeamercolor{structure}{fg=utablue} % itemize, enumerate, etc
\setbeamercolor{section in toc}{fg=utablue2} % TOC sections
\setbeamercolor{subsection in toc}{fg=utablue} 
\setbeamercolor{subsection in head/foot}{bg=utablue2,fg=utablue2}
\setbeamercolor{block title}{bg=utablue4,fg=black}
\setbeamercolor{title}{bg=utablue,fg=white}

\setbeamertemplate{enumerate items}[circle] % enumerates each item with a number inside a circle
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{section in toc}[circle]
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{frametitle}[default][left]
\setbeamertemplate{theorems}[numbered] % to number
\setbeamertemplate{navigation symbols}{\insertslidenavigationsymbol, }
\setbeamertemplate{background canvas}{
	\includegraphics[height = \paperheight, width = \paperwidth]{123_Page_04.png}
}
\setbeamertemplate{footline}[page number]{}
\setbeamertemplate{title page}{
	\vspace{6em}
	\centering
	\begin{beamercolorbox}[sep = 10pt, center, rounded = false]{title}
		\usebeamerfont{title}
		\inserttitle  
	\end{beamercolorbox}
	\vskip 1em \par
	\begin{beamercolorbox}[sep = 10pt, center]{author}
		\usebeamerfont{author}
		\insertauthor
	\end{beamercolorbox}
	\begin{beamercolorbox}[sep = 10pt, center]{institute}
		\usebeamerfont{institute}
		\insertinstitute
	\end{beamercolorbox}
	\vskip 1em \par
	\begin{beamercolorbox}[sep = 10pt, center]{date}
		\usebeamerfont{date}
		\insertdate
	\end{beamercolorbox}
}

 \AtBeginSection[]{
 	{\setbeamertemplate{background canvas}{\includegraphics[height=\paperheight,width=\paperwidth]{123_Page_02.png}}
	 	\begin{frame}<beamer>
	    	\frametitle{Section \thesection}
	    	\tableofcontents[currentsection]
	    \end{frame}
    }
}

\begin{document}

\title{Bayesian Hierarchical Dynamic Factor Models}
     
\author{
	Anthony M. Thomas, Jr.\\
	\footnotesize
	\href{mailto:anthony.thomas3@mavs.uta.edu}{anthony.thomas3@mavs.uta.edu}
}

\institute{
	Department of Mathematics\\ 
	The University of Texas at Arlington
}

\date{\today}

{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth]{UTA-title-page.png}}
\begin{frame}[plain]
	\titlepage
\end{frame}
}

{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth]{123_Page_02.png}}
\begin{frame}[plain]
\frametitle{Contents}
\tableofcontents
\end{frame}
}

\section{Background}
\subsection{Bayesian Inference}
\begin{frame}
	\frametitle{Bayesian Inference}
	\textbf{Bayesian Inference} can be described by two parts:
	\begin{enumerate}
		\item Build a model based on data $\bm X$ and parameters $\bm \Theta = \left\{ {\bm \Theta}_1, {\bm \Theta}_2 \right\}$
			\begin{itemize}
				\item[--] Likelihood: $p \left( {\bm X} | {\bm \Theta} \right)$
				\item[--] Prior: $p \left( {\bm \Theta}\right)$
			\end{itemize}
		\item Compute the posterior
			\begin{itemize}
				\item[--] Posterior: $$p \left( {\bm \Theta} | {\bm X} \right) = \frac{ p \left( {\bm X} | {\bm \Theta} \right) p \left( {\bm \Theta} \right) }{ p \left( {\bm X} \right)}$$
				\item[--] Report summaries, e.g. posterior expectations
				$$\E \left[ h( {\bm \Theta} ) | {\bm X} \right]$$
				or marginal posterior expectations
				$$\E \left[ h( {\bm \Theta}_i ) | {\bm X} \right]$$	
		\end{itemize}
	\end{enumerate}
\end{frame}

\subsection{Factor Analysis}

\begin{frame}
	\frametitle{Factor Analysis}
	\begin{itemize}
		\item \textbf{Factor Analysis} (FA) is a method that assumes that the covariance structure of a set of cross-sectional observations can be described in terms of a linear combination of latent variables called factors
		\item A sample of $P$ observations are related to set of factors through the equation
			\begin{equation}
				{\bm X}_i = {\bm \Lambda} {\bm F}_i + {\bm e}_i, \qquad i = 1, \ldots, P
			\end{equation}
			where 
			\begin{itemize}
				\item[--] ${\bm X}_i = \left(X_{1i}, \ldots, X_{Ni} \right)^{\top}$ denotes a vector of observations for variable $i$
				\item[--] ${\bm F}_i = \left(F_{1i}, \ldots, F_{Ki} \right)^{\top}$ denotes a vector of factors for variable $i$
				\item[--] ${\bm e}_i = \left(e_{1i}, \ldots, e_{Ni} \right)^{\top}$ denotes a vector of measurement errors and idiosyncratic (unique) factors for variable $i$
				\item[--] ${\bm \Lambda} = \left[ \lambda_{nk} \right]_{N \times K}$ denotes a matrix of factor loadings
			\end{itemize}
	\end{itemize}
\end{frame}
\begin{frame}
	\begin{itemize}
		\item The following assumptions are typically made in FA:			
			\begin{enumerate}
				\item $\rank \left( {\bm \Lambda} \right) = K$ 
				\item $\E \left[ {\bm X}_i \right] = \E \left[ {\bm e_i} \right] ={\bf 0}_N$ and $\E \left[ {\bm F_i} \right] ={\bf 0}_K \quad \forall i$
				\item $\Var \left( {\bm F}_i \right) = {\bf I}_K$ and $\Var \left( {\bm e}_i \right) = {\bm \Sigma}$ where ${\bm \Sigma} = \text{diag} \left( \sigma_1^2, \ldots, \sigma_N^2 \right) \quad \forall i$
				\item $\Cov \left( {\bm F}_i, {\bm e}_i \right) = {\bf 0}_{K \times N} \quad \forall i$
			\end{enumerate}
		\item Under these assumptions it follows that 
			\begin{equation*}
				\Var \left( {\bm X}_i \right)= {\bm \Lambda} {\bm \Lambda}^{\top} + {\bm \Sigma} \quad \forall i
			\end{equation*}
		\item Typical uses of FA:
			\begin{enumerate}
				\item Dimension reduction: explain the covariation between $N$ variables using $K < N$ factors
				\item Data interpretation: find factors that explain the covariation
				\item Theory testing: test whether a hypothesized factor structure fits observed data
			\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Time Series Factor Analysis}
	\begin{itemize}
		\item In \cite{gilbert2005time} FA is extended to time series data as \textbf{time series factor analysis} (TSFA)
		\item A sample of $T$ time series observations are related to the factors through the equation
			\begin{equation}
				{\bm X}_t = {\bm \Lambda} {\bm F}_t + {\bm e}_t \qquad t = 1, \ldots, T
			\end{equation}
			where
			\begin{itemize}
				\item[--] ${\bm X}_t = \left( X_{1t}, \ldots, X_{Nt} \right)^{\top}$ denotes a vector of observations at time $t$
				\item[--] ${\bm F}_t = \left( F_{1t}, \ldots, F_{Kt} \right)^{\top}$ denotes a vector of factors at time $t$
				\item[--] ${\bm e}_t$ is a vector of measurement errors and idiosyncratic factors at time $t$
				\item[--] ${\bm \Lambda} = \left[ \lambda_{nk} \right]_{N \times K}$ denotes a matrix of factor loadings
			\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Dynamic Factor Analysis}
\begin{frame}
	\frametitle{Dynamic Factor Analysis}
	\begin{itemize}
		\item In \textbf{dynamic factor analysis} (DFA) the factors are assumed to not only affect the observations contemporaneously, but affect them through their lags as well:
			\begin{equation}
				X_{nt} = {\bm \lambda}^n \left( L \right) {\bm F}_t + {\bm e}_t \quad n = 1, \ldots, N
			\end{equation}
			where 
			\begin{equation*}
				{\bm \lambda}^n \left( L \right) = {\bm \lambda}^n_0 + {\bm \lambda}^n_1 L + \cdots + {\bm \lambda}^n_q L^q
			\end{equation*}
			\begin{equation*}
				L^s {\bm F}_t = {\bm F}_{t-s} \quad \forall s \geq 0
			\end{equation*}
			is a distributed lag polynomial of factor loadings in the lag operator $L$ for the $n$th series
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Dynamic Factor Analysis}
	\begin{itemize}
		\item In DFA the factors are modeled as a time series process
		\item The time series process is commonly taken to be a vector autoregressive process, i.e.
			\begin{equation}
				{\bm \Psi} \left( L \right) {\bm F}_t = {\bm \varepsilon}_t
			\end{equation}
			where 
			\begin{equation*}
				{\bm \Psi} \left( L \right) = {\bf I}_K - {\bm \Psi}_1 L - \cdots  - {\bm \Psi}_p L^p
			\end{equation*}
			is a matrix polynomial of autoregressive coefficients in the lag operator $L$
	\end{itemize}
\end{frame}

\section{Hierarchical Dynamic Factor Analysis}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item In \cite{moenchDynamicHierarchicalFactor2013}, the authors generalize the dynamic factor model by positing that for each $t$, the $n$th series in a given block $b$, denoted by $ X_{bnt}$, has three sources of variation:
			\begin{enumerate}
				\item idiosyncratic
				\item block-specific
				\item common
			\end{enumerate}
		\tiem
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item A three-level representation of the data for $b = 1, \ldots, B$ is given as 
			\begin{align}
				X_{bnt} &= {\bm \lambda}^n_{G.b} \left( L \right) {\bm G}_{bt} + e_{Xbnt} \\
				{\bm G}_{bt} &= {\bm \Lambda}_{F.b} \left( L \right) {\bm F}_t + {\bm e}_{Gbt} \\
				{\bm \Psi}_F \left( L \right) {\bm F}_t &= {\bm \epsilon}_{Ft},
			\end{align}
			where
			\begin{itemize}
				\item[--] ${\bm \lambda}^n_{G.b} \left( L \right)$ denotes a distributed lag polynomial of block-level factor loadings
				\item[--] ${\bm \Lambda}_{F.b} \left( L \right)$ denotes a distributed lag matrix polynomial of common factor loadings
				\item[--] ${\bm G}_{bt} = \left( G_{b1t}, \ldots, G_{bK_{Gb}t} \right)^{\top}$ denotes the block-level factors
				\item[--] ${\bm F}_t = \left( F_{1t}, \ldots, F_{K_F t} \right)^{\top}$ denotes the common factors
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item For some blocks, it may be appropriate to break up the data into subblocks, which adds another source of variation
		\item Let $Z_{bsnt}$ be the $n$th series in subblock $s$ of block $b$
		\item A four-level representation of the subblock data is given as
			\begin{align*}
				Z_{bsnt} &= {\bm \lambda}^n_{H.bs} \left( L \right) {\bm H}_{bst} + e_{Zbsnt} \\
				{\bm H}_{bst} &= {\bm \Lambda}_{G.bs} \left( L \right) {\bm G}_{bt} + {\bm e}_{Hbst} \\
				{\bm G}_{bt} &= {\bm \Lambda}_{F.b} \left( L \right) {\bm F}_t + {\bm e}_{Gbt} \\
				{\bm \Psi}_F \left( L \right) {\bm F}_t &= {\bm \epsilon}_{Ft}
			\end{align*}	
			where
			\begin{itemize}
				\item[--] ${\bm \lambda}^n_{H.bs} \left( L \right)$ denotes a distributed lag polynomial of subblock-level factor loadings
				\item[--] ${\bm \Lambda}_{G.bs} \left( L \right)$ denotes a distributed lag matrix polynomial of block-level factor loadings
				\item[--] ${\bm H}_{bst} = \left( H_{bs1t}, \ldots, H_{bsK_{Hbs}t} \right)^{\top}$ denotes the subblock-level factors
			\end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item The idiosyncratic components, the subblock-specific, block-specific, and common factors are assumed to be stationary, Gaussian autoregressive processes of orders $q_{Zbsn}$, $q_{Xbn}$, $q_{Hbsi}$, $q_{Gbj}$, and $q_{Fk}$, respectively, i.e.
			\begin{alignat*}{3}
				\psi_{Z.bsn} \left( L \right) e_{Zbsnt} &= \epsilon_{Zbsnt}, \quad & \epsilon_{Zbsnt} &\sim \mathcal{N} \left( 0, \sigma^2_{Zbsn} \right) \quad & n &= 1, \ldots, N_{bs} \\
				\psi_{X.bn} \left( L \right) e_{Xbnt} &= \epsilon_{Xbnt}, \quad & \epsilon_{Xbnt} &\sim \mathcal{N} \left( 0, \sigma^2_{Xbn} \right) \quad & n &= 1, \ldots, N_b \\
				\psi_{H.bsi} \left( L \right) e_{Hbsit} &= \epsilon_{Hbsit}, & \epsilon_{Hbsi} &\sim \mathcal{N} \left(0, \sigma^2_{Hbsi} \right) & i &= 1, \ldots, K_{Hbs} \\
				\psi_{G.bj} \left( L \right) e_{Gbjt} &= \epsilon_{Gbjt}, & \epsilon_{Gbjt} &\sim \mathcal{N} \left(0, \sigma^2_{Gbj} \right) & j &= 1, \ldots, K_{Gb} \\
				\psi_{F.k} \left( L \right) F_{kt} &= \epsilon_{Fkt}, & \epsilon_{Fkt} &\sim \mathcal{N} \left(0, \sigma^2_{Fk} \right) & k &= 1, \ldots, K_{F}
			\end{alignat*}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item Not all series need to belong to blocks and subblocks, hence the data used in a four-level model are a mixture of $Z_{bsnt}$, $X_{bnt}$, and $X_{nt}$
		\item For brevity, we work with only the three-level model for which all data belongs to a block, therefore data will consist of $X_{bnt}$ for $b = 1, \ldots, B$
	\end{itemize}	
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item In \cite{moenchDynamicHierarchicalFactor2013} they develop a framework to estimate the posterior distribution of the factors and model parameters
		\item The main steps are outlined:
			\begin{algorithm}[H]
				\SetKwInput{Input}{Input}
				\SetKwInput{Output}{Output}
				\SetKwInput{Initialize}{Initialize}
				\SetAlgoLined
				\DontPrintSemicolon
				\Input{Data $\left\{ {\bm X}_{t} \right\}$}
				\Output{Sample from posterior distribution over factors and model parameters}
				\Initialize{ $\left\{ {\bm F}_{t} \right\}^{(0)} ,\left\{ {\bm G}_{t} \right\}^{(0)}, \left\{ {\bm \Lambda}, {\bm \Psi}, {\bm \Sigma} \right\}^{(0)}$}
				\For{$i = 1, \ldots, M$}{
					$\left\{ {\bm G}_{t} \right\}^{(i)} \sim \left\{ {\bm G}_{t} \right\} | \left\{ {\bm F}_t \right\}^{(i-1)}, \left\{ {\bm \Lambda}, {\bm \Psi}, {\bm \Sigma} \right\}^{(i-1)}, \left\{ {\bm X}_{t} \right\}$ \vspace{0.5em}\;
					$\left\{ {\bm F}_{t} \right\}^{(i)} \sim \left\{ {\bm F}_{t} \right\} | \left\{ {\bm G}_{t} \right\}^{(i)}, \left\{ {\bm \Lambda}, {\bm \Psi}, {\bm \Sigma} \right\}^{(i-1)}, \left\{ {\bm X}_{t} \right\}$\vspace{0.5em}\;
					$\left\{ {\bm \Lambda}, {\bm \Psi}, {\bm \Sigma} \right\}^{(i)} \sim {\bm \Lambda}, {\bm \Psi}, {\bm \Sigma} | \left\{ {\bm F}_{t} \right\}^{(i)}, \left\{ {\bm G}_{t} \right\}^{(i)}, \left\{ {\bm X}_{t} \right\}$\;
				}
			  	\Return $\left\{ \left\{ {\bm F}_t \right\}^{(i)}, \left\{ {\bm G}_{t} \right\}^{(i)}, \left\{ {\bm \Lambda}, {\bm \Psi}, {\bm \Sigma} \right\}^{(i)} \right\}_{i = 1}^M$ \;
			 	\caption{Gibbs Sampler for a Hierarchical DFM}
			\end{algorithm}
	\end{itemize}
\end{frame}

\section{Variational Bayesian Inference}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	\begin{itemize}
		\item Consider a model with data ${\bm X}$ and latent variables ${\bm Z}$
		\item The goal is to compute the joint posterior of the latent variables given the data
	  		\begin{equation} \label{posterior}
	  			p \left( {\bm Z} | {\bm X} \right) = \frac{ p \left( {\bm X} | {\bm Z} \right) p \left( {\bm Z} \right) }{ p \left( {\bm X} \right) }
	  		\end{equation}
 		  	\begin{itemize}
	  			\item[--] Likelihood: $p \left( {\bm X} | {\bm Z} \right)$
				\item[--] Prior: $p \left( {\bm Z}\right)$
				\item[--] Evidence: $p \left( {\bm X} \right)$
					\begin{equation} 			
						p \left( {\bm X} \right) = \int p \left( {\bm X}, {\bm Z} \right) \, d{\bm Z} \label{marginal.evidence}
					\end{equation}
			\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	\begin{itemize}
		\item For complex models (\ref{posterior}) and (\ref{marginal.evidence}) either have no closed-form or require high-dimensional integration which causes the ``inference problem"
		\item As a result, the joint posterior has to be approximated
		\item Markov chain Monte Carlo (MCMC) has been the gold standard to solve this problem
			\begin{enumerate}
				\item Construct an ergodic Markov chain on $\bm Z$ whose stationary distribution is the joint posterior $p \left( {\bm Z} | {\bm X} \right)$
				\item Sample from the chain to collect samples from the stationary distribution
				\item Approximate the posterior with an empirical estimate constructed from a subset of the collected samples
				\item Use the subset of collected samples to estimate expectations of interest
			\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	\begin{itemize}
		\item A major disadvantage for MCMC is that, while it is eventually accurate, it often takes a long time to obtain results
		\item Variational Bayes (VB) typical obtains results much faster
		\item While MCMC uses sampling to solve the inference problem, VB instead uses optimization
			
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	
	\begin{enumerate}
		\item Posit a family of ``nice" approximate densities $\mathcal Q$
		\item Find a member of that family that is ``closest" to the exact posterior
	\end{enumerate}	
		\begin{equation}
			q^{\star} \left( {\bm Z} \right) = \argmin_{q \left( {\bm Z} \right) \in \mathcal Q} \text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) \label{objective}
		\end{equation}
		where 
		\begin{equation}
			\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \E_q \left[ \log q \left( {\bm Z} \right)\right] - \E_q \left[ \log p \left( {\bm Z} | {\bm X} \right) \right]
		\end{equation}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	It follows that 
	\begin{equation*}
		\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \E_q \left[ \log q \left( {\bm Z} \right)\right] - \E_q \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] + \log p \left( {\bm X} \right)
	\end{equation*}
	This reveals that the objective in (\ref{objective}) depends on the evidence, thus it cannot be computed directly.
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	Instead, we optimize an alternative objective that is equivalent to (\ref{objective}) up to an added constant called the \textbf{evidence lower bound} (ELBO)
	\begin{equation}
		\text{ELBO} \left( q \right) = \E_q \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] -\E_q \left[ \log q \left( {\bm Z} \right)\right] \label{ELBO}
	\end{equation}
	It can be shown that 
	\begin{equation*}
		\argmin_{q \left( {\bm Z} \right) \in \mathcal Q} \text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \argmax_{q \left( {\bm Z} \right) \in \mathcal Q} \text{ELBO} \left( q \right) 
	\end{equation*}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	The VB framework is now
	\begin{enumerate}
		\item Posit a family of ``nice" approximate densities $\mathcal Q$
		\item Find a member of that family that is ``closest" to the exact posterior, i.e.
		\begin{equation}
			q^{\star} \left( {\bm Z} \right) = \argmax_{q \left( {\bm Z} \right) \in \mathcal Q} \text{ELBO} \left( q \right) \label{new.objective}
		\end{equation}
	\end{enumerate}
	Solving this optimization problem is still difficult in general
		\begin{itemize}
			\item Using the mean-field assumption can make it easier
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Mean-Field Assumption}
	\begin{enumerate}
		\item Partition the latent variables into $M$ groups, say ${\bm Z}_1, \ldots, {\bm Z}_M$
		\item Assume that the distributions in $\mathcal Q$ factorize across the groups, i.e.
		$$\mathcal Q = \left\{ q : q \left( {\bm Z} \right) = \prod_{m = 1}^M q_m \left( {\bm Z}_m \right) \right\}$$
		\item Learning the optimal $q$ now reduces to learning the optimal $q_1, \ldots, q_M$
		\item Straightforward to optimize via coordinate ascent
		\item This is \textbf{NOT} a modeling assumption
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Mean-Field Assumption}
	Interestingly, under the mean-field assumption, the optimization problem for  a single $q_m$ has the solution:
	\begin{equation}
		q_m \left( {\bm Z}_m \right) = \frac{ \exp \left\{ \E_{-m} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\} }{ \int \exp \left\{ \E_{-m} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\} \,d{\bm Z}_m }
	\end{equation}
	This establishes what is called the \textbf{coordinate ascent variational inference} algorithm.
\end{frame}

\begin{frame}
	\frametitle{Coordinate Ascent Variational Inference}
	\begin{algorithm}[H]
		\SetKwInput{Input}{Input}
		\SetKwInput{Output}{Output}
		\SetKwInput{Initialize}{Initialize}
		\SetAlgoLined
		\DontPrintSemicolon
		\Input{ Model $p \left( {\bm X}, {\bm Z} \right)$,  Data ${\bm X}$, }
		\Output{Variational density  $q \left( {\bm Z} \right) = \prod_{m=1}^M q_m \left( {\bm Z}_m \right)$}
		\Initialize{Variational densities $q_m \left( {\bm Z}_m \right)$}
		\While{the ELBO has not converged}{
			\For{$m \in \left\{ 1, 2, \ldots, M \right\}$}{
				Set  $q_m \left( {\bm Z}_m \right) \propto \exp \left\{ \E_{-m} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\}$\;
			}
			Compute $\operatorname{ELBO} \left( q \right)$
	  	}
	  	\Return $q \left( {\bm Z} \right)$ \;
	 	\caption{Coordinate ascent variational inference}
	\end{algorithm}
\end{frame}

\subsection{Orthogonal Factor Model}
\begin{frame}
	\frametitle{The Orthogonal Factor Model}
	Here we present a VB framework for the orthogonal time series factor analysis model.
	The orthogonal FA model assumes the form 
	\begin{equation*}
		{\bm X}_t = {\bm \Lambda} {\bm F}_t+ {\bm e}_t
	\end{equation*}
	with the following assumptions:
	\begin{enumerate}
		\item ${\bm F}_t \overset{\text{iid}}{\sim} \mathcal N \left( {\bm 0}_K, \bf{I}_K \right)$
		\item ${\bm e}_t \overset{\text{iid}}{\sim} \mathcal N \left( {\bm 0}_N, \bm \Sigma \right)$ where $\bm \Sigma = \operatorname{diag} \left( \sigma_1^2, \ldots, \sigma_N^2 \right)$
		\item ${\bm F}_t$ and ${\bm e}_s$ are independent for every pair $t, s$
	\end{enumerate}
	where ${\bm 0}_K$ and ${\bm 0}_N$ are zero-vectors of lengths $K$ and $N$, respectively, and ${\bf I}_K$ is the $K \times K$ identity matrix.
\end{frame}

\begin{frame}
	Under the assumptions in the previous slide the likelihood is
	\begin{equation*}
		p \left( {\bf X}_{1:T} | {\bf F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \propto \det \left( {\bf \Sigma} \right)^{-\frac{T}{2}} \exp \left\{ -\frac{1}{2} \sum_{t = 1}^T \left( {\bf X}_t - {\bm \Lambda} {\bf F}_t \right)^{\top} {\bf \Sigma}^{-1} \left( {\bf X}_t - {\bm \Lambda} {\bf F}_t \right) \right\}
	\end{equation*}
	We assign the following prior distributions:
	\begin{align*}
		p \left( {\bm \Lambda} | {\bm \Sigma} \right) &= \prod_{n = 1}^N \mathcal{N} \left( {\bm \lambda}_n \,|\, {\bf 0}_K, \sigma^2_n {\bf I}_K \right) \\
		p \left( {\bm \Sigma} \right) &= \prod_{n = 1}^N \text{Scale-inv-} \chi^2 \left( \sigma^2_n \,|\, \nu_0, \tau^2_0 \right)
	\end{align*}
\end{frame}

\begin{frame}
	The goal here is to find a variational approximation to the posterior distribution over using CAVI, i.e. to find
	\begin{equation*}
		q^{\star} \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \approx p \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} | {\bm X}_{1:T} \right)
	\end{equation*}
	such that 
	\begin{equation*}
		q^{\star} \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) = \argmax_{q \left( {\bm F}, {\bm \Lambda}, {\bm \Sigma}  \right) \in \mathcal Q} \text{ELBO} \left( q \right) \label{new.objective}
	\end{equation*}
	where
	\begin{align*}
		\mathcal Q &= \left\{ q : q \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) = q \left( {\bm F}_{1:T} \right) q \left( {\bm \Lambda} \right) q \left( {\bm \Sigma} \right) \right\} \\
		&= \left\{ q : q \left( {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) = \prod_{t = 1}^T q \left( {\bm F}_t \right) \prod_{n = 1}^N q \left( {\bm \lambda}_n \right) q \left( {\sigma}^2_n \right) \right\}
	\end{align*}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item With the mean-field assumption we have 
			\begin{align*}
				\log q \left( {\bm F}_{1:T} \right) &= \E_{-{\bm F}} \left[ \log p \left( {\bm X}_{1:T}, {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \right] + \text{const} \\
				\log q \left( {\bm \Lambda} \right) &= \E_{-{\bm \Lambda}} \left[ \log p \left( {\bm X}_{1:T}, {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \right] + \text{const} \\
				\log q \left( {\bm \Sigma} \right) &= \E_{-{\bm \Sigma}} \left[ \log p \left( {\bm X}_{1:T}, {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) \right] + \text{const}
			\end{align*}
		\item In this case the log-joint $\log p \left( {\bm X}_{1:T}, {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right)$ expands as 
			\begin{equation*}
				\log p \left( {\bm X}_{1:T} | {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) + \log p \left( {\bm F}_{1:T} \right) + \log p \left( {\bm \Lambda} | {\bm \Sigma} \right) + \log p \left( {\bm \Sigma} \right)
			\end{equation*}
		\item Therefore,
			\begin{align*}
				\log q \left( {\bm F}_{1:T} \right) &= \E_{-{\bm F}} \left[ \log p \left( {\bm X}_{1:T} | {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) + \log p \left( {\bm F}_{1:T} \right) \right] + \text{const} \\
				\log q \left( {\bm \Lambda} \right) &= \E_{-{\bm \Lambda}} \left[ \log p \left( {\bm X}_{1:T} | {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) + \log p \left( {\bm \Lambda} | {\bm \Sigma} \right) \right] + \text{const} \\
				\log q \left( {\bm \Sigma} \right) &= \E_{-{\bm \Sigma}} \left[ \log p \left( {\bm X}_{1:T} | {\bm F}_{1:T}, {\bm \Lambda}, {\bm \Sigma} \right) + \log p \left( {\bm \Lambda} | {\bm \Sigma} \right) + \log p \left( {\bm \Sigma} \right) \right] + \text{const}
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Substituting in the expressions for the likelihood and prior for ${\bm F}_{1:T}$ we get
			\begin{equation*}
				\log q \left( {\bm F}_{1:T} \right) = - \frac{1}{2} \sum_{t = 1}^T {\bm F}_t^{\top} \left( {\bf I}_K + \E_{-{\bm F}} \left[ \Lambda^{\top} \bm \Sigma^{-1} \bm \Lambda \right] \right) {\bm F}_t + \sum_{t = 1}^T {\bf X}_t^{\top} \E_{-{\bm F}} \left[ {\bm \Sigma}^{-1} {\bm \Lambda} \right] {\bm F}_t + \text{const}
				\end{equation*}
		\item The equation above is Gaussian wrt ${\bm F}_t$. Thus,
			\begin{equation*}
				q \left( {\bm F}_{1:T} \right) = \prod_{t = 1}^T \mathcal{N} \left( {\bm F}_t \,|\, {\bf m}_{{\bm F} t}, {\bf P}_{\bm F} \right) 
			\end{equation*}
			where
			\begin{align*}
				{\bf P}_{\bm F}^{-1} &= {\bf I}_{K} + \sum_{n = 1}^N \E_{{\bm \Sigma}} \left[ \frac{1}{\sigma^2_n} \right] \E_{{\bm \Lambda}} \left[ {\bm \lambda}_n {\bm \lambda}_n^{\top} \right] \\
				{\bf m}_{{\bm F} t} &= {\bf P}_{\bm F} \, \E_{{\bm \Sigma}} \left[ {\bm \Sigma}^{-1} \right] \E_{{\bm \Lambda}} \left[ {\bm \Lambda} \right] {\bm X}_t
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item Proceeding in a similar way, we have that
			\begin{align*}
				q(\bm \Lambda) &= \prod_{n = 1}^N \mathcal{N} \left( {\bm \lambda}_n \,|\, {\bf m}_{{\bm \lambda} n}, {\bf P}_{{\bm \lambda} n} \right) \\
				q({\bm \Sigma}) &= \prod_{n = 1}^N \text{Scale-inv-}\chi^2 \left( \sigma_n^2 \,|\, \nu_{\sigma}, \tau_n^2 \right)
			\end{align*}
			where
			\vspace{-1em}
			\begin{align*}
				{\bf P}_{\bm \lambda n}^{-1} &= \E_{{\bm \Sigma}} \left[ \frac{1}{\sigma_n^2} \right] \left( T \, {\bf I}_{K} + \sum_{t = 1}^T \E_{{\bm F}} \left[ {\bm F}_t {\bm F}_t^{\top} \right] \right) \\
				{\bf m}_{\bm \lambda n} &= {\bf P}_{\bm \lambda n}  \E_{{\bm \Sigma}} \left[ \frac{1}{\sigma_n^2} \right] \sum_{t = 1}^T X_{nt} \E_{{\bm F}} \left[ {\bm F}_t \right] \\
				\nu_{\sigma} &= T + \nu_0\\
				\nu_{\sigma} \tau_n^2 &= \nu_0 \tau_0^2 + \E_{{\bm \Lambda}} \left[ {\bm \lambda}_n^{\top} {\bm \lambda}_n \right] + \sum_{t = 1}^T \left[ X_{nt}^2 - 2 X_{nt} \E_{{\bm \Lambda}} \left[ {\bm \lambda}_n \right]^{\top} \E_{{\bm F}} \left[ {\bm F}_t \right] + \tr \left[ \E_{{\bm \Lambda}} \left[ {\bm \lambda}_n {\bm \lambda}_n^{\top} \right] \E_{{\bm F}} \left[ {\bm F}_t {\bm F}_t^{\top} \right] \right] \right]
			\end{align*}
	\end{itemize}
\end{frame}

\section{Future Work}
\begin{frame}
	\frametitle{Future Work}
	The main goal of the project is to develop a variational Bayesian framework to handle the four-level model from \cite{moenchDynamicHierarchicalFactor2013}.
\end{frame}
\section{References}
\begin{frame}[allowframebreaks]
	\frametitle{References}
	\printbibliography
\end{frame}

\end{document}