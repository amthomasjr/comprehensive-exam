\documentclass[10pt, compress, notheorems]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm, amsmath, mathrsfs, amsfonts, bm}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage{breqn}
\usepackage[backend=biber]{biblatex}
\usepackage{tabulary}
\usepackage{multirow}
\usepackage{filecontents}
\usepackage{comment}

\usefonttheme{serif}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose \bgroup \originalleft}
\renewcommand{\right}{\aftergroup \egroup \originalright}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% \hypersetup{colorlinks=true} % Enable colored hyperlinks

\definecolor{utablue}{RGB}{0,100,177}% Official RGB code for UTA blue 
\definecolor{utaorange}{RGB}{245,128,38} 
\definecolor{utablue2}{RGB}{0,68,124}
\definecolor{utablue3}{RGB}{212,239,252}
\definecolor{utablue4}{RGB}{231,246,253}%R-231 G-246 B-253

\setbeamercolor{palette primary}{bg=utablue2,fg=utablue2}
\setbeamercolor{palette secondary}{bg=utablue2,fg=white} %footer color
\setbeamercolor{palette tertiary}{bg=black,fg=utablue2}% header color
\setbeamercolor{palette quaternary}{bg=utablue2,fg=black}
\setbeamercolor{structure}{fg=utablue} % itemize, enumerate, etc
\setbeamercolor{section in toc}{fg=utablue2} % TOC sections
\setbeamercolor{subsection in toc}{fg=utablue} 
\setbeamercolor{subsection in head/foot}{bg=utablue2,fg=utablue2}
\setbeamercolor{block title}{bg=utablue4,fg=black}
\setbeamercolor{title}{bg=utablue,fg=white}

\setbeamertemplate{enumerate items}[circle] % enumerates each item with a number inside a circle
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{section in toc}[circle]
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{frametitle}[default][left]
\setbeamertemplate{theorems}[numbered] % to number
\setbeamertemplate{navigation symbols}{\insertslidenavigationsymbol, }
\setbeamertemplate{background canvas}{
	\includegraphics[height = \paperheight, width = \paperwidth]{123_Page_04.png}
}
\setbeamertemplate{footline}[page number]{}
\setbeamertemplate{title page}{
	\vspace{6em}
	\centering
	\begin{beamercolorbox}[sep = 10pt, center, rounded = false]{title}
		\usebeamerfont{title}
		\inserttitle  
	\end{beamercolorbox}
	\vskip 1em \par
	\begin{beamercolorbox}[sep = 10pt, center]{author}
		\usebeamerfont{author}
		\insertauthor
	\end{beamercolorbox}
	\begin{beamercolorbox}[sep = 10pt, center]{institute}
		\usebeamerfont{institute}
		\insertinstitute
	\end{beamercolorbox}
	\vskip 1em \par
	\begin{beamercolorbox}[sep = 10pt, center]{date}
		\usebeamerfont{date}
		\insertdate
	\end{beamercolorbox}
}

 \AtBeginSection[]{
 	{\setbeamertemplate{background canvas}{\includegraphics[height=\paperheight,width=\paperwidth]{123_Page_02.png}}
	 	\begin{frame}<beamer>
	    	\frametitle{Section \thesection}
	    	\tableofcontents[currentsection, currentsubsection]
	    \end{frame}
    }
}

\begin{document}

\title{Bayesian Hierarchical Dynamic Factor Models}
     
\author{
	Anthony M. Thomas, Jr.\\
	\footnotesize
	\href{mailto:anthony.thomas3@mavs.uta.edu}{anthony.thomas3@mavs.uta.edu}
}

\institute{
	Department of Mathematics\\ 
	The University of Texas at Arlington
}

\date{\today}

{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth]{UTA-title-page.png}}
\begin{frame}[plain]
	\titlepage
\end{frame}
}

{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth]{123_Page_02.png}}
\begin{frame}[plain]
\frametitle{Contents}
\tableofcontents
\end{frame}
}

\section{Background}
\begin{frame}
	\frametitle{Factor Analysis}
	\begin{enumerate}
		\item \textbf{Factor Analysis} is a method that uses the covariances between a set of observed variables to described them in terms of a smaller set of unobservable variables called factors.
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Bayesian Inference}
	\textbf{Bayesian Inference} can be described by two parts:
	\begin{enumerate}
		\item Build a model based on data $\bm X$ and parameters $\bm \Theta = \left\{ {\bm \Theta}_1, {\bm \Theta}_2 \right\}$
			\begin{itemize}
				\item Likelihood: $p \left( {\bm X} | {\bm \Theta} \right)$
				\item Prior: $p \left( {\bm \Theta}\right)$
			\end{itemize}
		\item Compute the posterior
			\begin{itemize}
				\item Posterior: $$p \left( {\bm \Theta} | {\bm X} \right) = \frac{ p \left( {\bm X} | {\bm \Theta} \right) p \left( {\bm \Theta} \right) }{ p \left( {\bm X} \right)}$$
				\item Can report summaries, e.g. posterior expectations
				$$\E \left[ h( {\bm \Theta} ) | {\bm X} \right]$$
				and compute marginal posteriors
				$$p \left( {\bm \Theta}_1 | {\bm X} \right) \text{ and } p \left( {\bm \Theta}_2 | {\bm X} \right)$$
			\end{itemize}
	\end{enumerate}
\end{frame}

\section{Variational Bayesian Inference}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	Consider a model with data $\bm X$ and unknowns $\bm Z = \left\{ {\bm Z}_1, {\bm Z}_2 \right\}$ (could be latent variables and model parameters).
  		The goal is to compute the joint posterior of the unknowns given the data
  		\begin{equation} \label{posterior}
  			p \left( {\bm Z} | {\bm X} \right) = \frac{ p \left( {\bm X} | {\bm Z} \right) p \left( {\bm Z} \right) }{ p \left( {\bm X} \right) }
  		\end{equation}
  		\begin{itemize}
  			\item Likelihood: $p \left( {\bm X} | {\bm Z} \right)$
			\item Prior: $p \left( {\bm Z}\right)$
			\item Evidence: $p \left( {\bm X} \right)$
			\begin{equation} 			
				p \left( {\bm X} \right) = \int p \left( {\bm X}, {\bm Z} \right) \, d{\bm Z} \label{marginal.evidence}
			\end{equation}
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	For complex models (\ref{posterior}) and (\ref{marginal.evidence}) typically have no closed-form.
	As a result, the joint and marginal posteriors have to be approximated.
	Markov chain Monte Carlo (MCMC) has been the gold standard to solve this problem.
	\begin{enumerate}
		\item Construct an ergodic Markov chain on $\bm Z$ whose stationary distribution is the joint posterior $p \left( {\bm Z} | {\bm X} \right)$
		\item Sample from the chain to collect samples from the stationary distribution
		\item Approximate the posterior with an empirical estimate constructed from a subset of the collected samples
		\item Use the subset of collected samples to estimate expectations of interest
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	A major disadvantage for MCMC is that high levels of accuracy requires a great deal of time
	\begin{enumerate}
		\item MCMC is typically slow, but ultimately accurate
		\item VB is typically much faster
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	Rather than use sampling, VB uses optimization to find an approximation to the posterior.
	\begin{enumerate}
		\item Posit a family of ``nice" approximate densities $\mathcal Q$
		\item Find a member of that family that is ``closest" to the exact posterior, i.e.
		\begin{equation}
			q^{\star} \left( {\bm Z} \right) = \argmin_{q \left( {\bm Z} \right) \in \mathcal Q} \text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) \label{objective}
		\end{equation}
		where 
		\begin{equation}
			\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \E_q \left[ \log q \left( {\bm Z} \right)\right] - \E_q \left[ \log p \left( {\bm Z} | {\bm X} \right) \right]
		\end{equation}
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	It can be shown that 
	\begin{equation*}
		\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \E_q \left[ \log q \left( {\bm Z} \right)\right] - \E_q \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] + \log p \left( {\bm X} \right)
	\end{equation*}
	This reveals that the objective in (\ref{objective}) depends on the evidence, thus it cannot be computed directly.
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	Instead, we optimize an alternative objective that is equivalent to (\ref{objective}) up to an added constant called the \textbf{evidence lower bound} (ELBO)
	\begin{equation}
		\text{ELBO} \left( q \right) = \E_q \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] -\E_q \left[ \log q \left( {\bm Z} \right)\right]	
		\end{equation}
	Maximizing the ELBO is equivalent to (\ref{objective}).
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	The VB framework is now
	\begin{enumerate}
		\item Posit a family of ``nice" approximate densities $\mathcal Q$
		\item Find a member of that family that is ``closest" to the exact posterior, i.e.
		\begin{equation}
			q^{\star} \left( {\bm Z} \right) = \argmax_{q \left( {\bm Z} \right) \in \mathcal Q} \text{ELBO} \left( q \right) \label{new.objective}
		\end{equation}
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Mean-Field Assumption}
	
\end{frame}

\section{Classical Factor Analysis}

\begin{frame}
	\frametitle{Classical Factor Analysis Model}	
	The Classical (orthogonal) FA model assumes assumes the form 
	\begin{equation*}
		{\bm X} = {\bm \Lambda} {\bm F}+ {\bm e}
	\end{equation*}
	where
	\begin{enumerate}
		\item ${\bm X} = \left( X_{1}, \ldots, X_{N} \right)^{\top}$ denotes the vector of observations
		\item $\bm \Lambda = \left[ \lambda_{nk} \right]_{N \times K}$ denotes the matrix of factor loadings 
		\item ${\bm F} = \left( F_{1}, \ldots, F_{K} \right)^{\top}$ denotes the vector of latent factors
		\item ${\bm e} = \left( e_{1}, \ldots, e_{N} \right)^{\top}$ denotes the vector of latent error terms
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Normal Theory Assumptions}
	The Normal Theory Classical FA model assumes the form
	\begin{equation*}
		{\bm X} = {\bm \Lambda} {\bm F}+ {\bm e}
	\end{equation*}
	and adds the assumptions that 
	\begin{enumerate}
		\item $\bm F \sim \mathcal N \left( \bm 0, \bm{I}_K \right)$
		\item $\bm e \sim \mathcal N \left( \bm 0, \bm \Sigma \right)$ where $\bm \Sigma = \operatorname{diag} \left( \sigma_1^2, \ldots, \sigma_N^2 \right)$
		\item $F_k$ and $e_n$ are independent for every pair $k, n$
	\end{enumerate}
\end{frame}

\section{Hierarchical Dynamic Factor Analysis}
\subsection{Dynamic Factor Analysis}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{align}
	{\bf X}_{bst} &= {\bm \Lambda}_{H.bs} (L) {\bf H}_{bst} + {\bf e}_{Xbst} \\
	{\bf H}_{bst} &= {\bm \Lambda}_{G.bs} (L) {\bf G}_{bt} + {\bf e}_{Hbst} \\
	{\bf G}_{bt} &= {\bm \Lambda}_{F.b} (L) {\bf F}_{t} + {\bf e}_{Gbt} \\
	{\bm \Psi}_{F} (L) {\bf F}_{t} &= {\bm \epsilon}_{Ft},
\end{align}
\end{frame}

\end{document}