\documentclass[10pt, compress, notheorems]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm, amsmath, mathrsfs, amsfonts, bm}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage{breqn}
\usepackage[backend=biber]{biblatex}
	\addbibresource{ref.bib}
\usepackage{tabulary}
\usepackage{multirow}
\usepackage{filecontents}
\usepackage{comment}

\usefonttheme{serif}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose \bgroup \originalleft}
\renewcommand{\right}{\aftergroup \egroup \originalright}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% \hypersetup{colorlinks=true} % Enable colored hyperlinks

\definecolor{utablue}{RGB}{0,100,177}% Official RGB code for UTA blue 
\definecolor{utaorange}{RGB}{245,128,38} 
\definecolor{utablue2}{RGB}{0,68,124}
\definecolor{utablue3}{RGB}{212,239,252}
\definecolor{utablue4}{RGB}{231,246,253}%R-231 G-246 B-253

\setbeamercolor{palette primary}{bg=utablue2,fg=utablue2}
\setbeamercolor{palette secondary}{bg=utablue2,fg=white} %footer color
\setbeamercolor{palette tertiary}{bg=black,fg=utablue2}% header color
\setbeamercolor{palette quaternary}{bg=utablue2,fg=black}
\setbeamercolor{structure}{fg=utablue} % itemize, enumerate, etc
\setbeamercolor{section in toc}{fg=utablue2} % TOC sections
\setbeamercolor{subsection in toc}{fg=utablue} 
\setbeamercolor{subsection in head/foot}{bg=utablue2,fg=utablue2}
\setbeamercolor{block title}{bg=utablue4,fg=black}
\setbeamercolor{title}{bg=utablue,fg=white}

\setbeamertemplate{enumerate items}[circle] % enumerates each item with a number inside a circle
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{section in toc}[circle]
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{frametitle}[default][left]
\setbeamertemplate{theorems}[numbered] % to number
\setbeamertemplate{navigation symbols}{\insertslidenavigationsymbol, }
\setbeamertemplate{background canvas}{
	\includegraphics[height = \paperheight, width = \paperwidth]{123_Page_04.png}
}
\setbeamertemplate{footline}[page number]{}
\setbeamertemplate{title page}{
	\vspace{6em}
	\centering
	\begin{beamercolorbox}[sep = 10pt, center, rounded = false]{title}
		\usebeamerfont{title}
		\inserttitle  
	\end{beamercolorbox}
	\vskip 1em \par
	\begin{beamercolorbox}[sep = 10pt, center]{author}
		\usebeamerfont{author}
		\insertauthor
	\end{beamercolorbox}
	\begin{beamercolorbox}[sep = 10pt, center]{institute}
		\usebeamerfont{institute}
		\insertinstitute
	\end{beamercolorbox}
	\vskip 1em \par
	\begin{beamercolorbox}[sep = 10pt, center]{date}
		\usebeamerfont{date}
		\insertdate
	\end{beamercolorbox}
}

 \AtBeginSection[]{
 	{\setbeamertemplate{background canvas}{\includegraphics[height=\paperheight,width=\paperwidth]{123_Page_02.png}}
	 	\begin{frame}<beamer>
	    	\frametitle{Section \thesection}
	    	\tableofcontents[currentsection]
	    \end{frame}
    }
}

\begin{document}

\title{Bayesian Hierarchical Dynamic Factor Models}
     
\author{
	Anthony M. Thomas, Jr.\\
	\footnotesize
	\href{mailto:anthony.thomas3@mavs.uta.edu}{anthony.thomas3@mavs.uta.edu}
}

\institute{
	Department of Mathematics\\ 
	The University of Texas at Arlington
}

\date{\today}

{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth]{UTA-title-page.png}}
\begin{frame}[plain]
	\titlepage
\end{frame}
}

{
\setbeamertemplate{background canvas}{\includegraphics[width=\paperwidth]{123_Page_02.png}}
\begin{frame}[plain]
\frametitle{Contents}
\tableofcontents
\end{frame}
}

\section{Background}
\subsection{Bayesian Inference}
\begin{frame}
	\frametitle{Bayesian Inference}
	\textbf{Bayesian Inference} can be described by two parts:
	\begin{enumerate}
		\item Build a model based on data $\bm X$ and parameters $\bm \Theta = \left\{ {\bm \Theta}_1, {\bm \Theta}_2 \right\}$
			\begin{itemize}
				\item Likelihood: $p \left( {\bm X} | {\bm \Theta} \right)$
				\item Prior: $p \left( {\bm \Theta}\right)$
			\end{itemize}
		\item Compute the posterior
			\begin{itemize}
				\item Posterior: $$p \left( {\bm \Theta} | {\bm X} \right) = \frac{ p \left( {\bm X} | {\bm \Theta} \right) p \left( {\bm \Theta} \right) }{ p \left( {\bm X} \right)}$$
				\item Report summaries, e.g. posterior expectations
				$$\E \left[ h( {\bm \Theta} ) | {\bm X} \right]$$
				or marginal posterior expectations
				$$\E \left[ h( {\bm \Theta}_i ) | {\bm X} \right]$$	
		\end{itemize}
	\end{enumerate}
\end{frame}

\subsection{Factor Analysis}

\begin{frame}
	\frametitle{Factor Analysis}
	\textbf{Factor Analysis} (FA) is a method that assumes that the covariance structure of a set of observations can be described in terms of a linear combination of latent variables called factors. \bigskip
	
	Typical uses of FA:
	\begin{enumerate}
		\item Dimension reduction: explains covariation between $N$ variables using $K < N$ factors
		\item Data interpretation: finds factors that explain the covariation
		\item Theory testing: tests whether a hypothesized factor structure fits observed data
	\end{enumerate}
\end{frame}

\subsection{Dynamic Factor Analysis}
\begin{frame}
	\frametitle{Dynamic Factor Analysis}
	\textbf{Dynamic factor models} allow the factors to not only affect the observations contemporaneously, but also through their lags:
	\begin{equation}
		{\bm X}_t = {\bm \Lambda} \left( L \right) {\bm F}_t + {\bm \epsilon}_t
	\end{equation}
	where 
	\begin{equation*}
		{\bm \Lambda} \left( L \right) = {\bm \Lambda}_0 + {\bm \Lambda}_1 L + \cdots + {\bm \Lambda}_s L^s
	\end{equation*}
	is a distributed lag matrix polynomial of factor loadings in the lag operator $L$ and
	\begin{equation*}
		L^s {\bm F}_t = {\bm F}_{t-s}
	\end{equation*}	
\end{frame}

\begin{frame}
	\frametitle{Dynamic Factor Analysis}
	The factors are usually assumed to follow a vector autoregressive process:
	\begin{equation}
		{\bm \Phi} \left( L \right) {\bm F}_t = {\bm \varepsilon}_t
	\end{equation}
	where 
	\begin{equation*}
		{\bm \Phi} \left( L \right) = {\bf I}_K - {\bm \Phi}_1 L - \cdots  - {\bm \Phi}_p L^p
	\end{equation*}
	is a matrix polynomial of autoregressive coefficients in the lag operator $L$.
\end{frame}

\section{Hierarchical Dynamic Factor Analysis}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	In \cite{moenchDynamicHierarchicalFactor2013}, the authors generalize the two-level dynamic factor model by positing that for each $t$, the $n$th series in a given block $b$, denoted by $ X_{bnt}$, has three sources of variation:
	\begin{enumerate}
		\item idiosyncratic
		\item block-specific
		\item common
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	A three-level representation of the data is given as 
	\begin{align}
		X_{bnt} &= {\bm \lambda}^n_{G.b} \left( L \right) {\bm G}_{bt} + e_{Xbnt} \\
		{\bm G}_{bt} &= {\bm \Lambda}_{F.b} \left( L \right) {\bm F}_t + {\bm e}_{Gbt} \\
		{\bm \Psi}_F \left( L \right) {\bm F}_t &= {\bm \epsilon}_{Ft},
	\end{align}
	where
	\begin{itemize}
		\item ${\bm \lambda}^n_{G.b} \left( L \right)$ denotes a distributed lag polynomial of block-level factor loadings
		\item ${\bm \Lambda}_{F.b} \left( L \right)$ denotes a distributed lag matrix polynomial of common factor loadings
		\item ${\bm G}_{bt} = \left( G_{b1t}, \ldots, G_{bK_{Gb}t} \right)^{\top}$ denotes the block-level factors
		\item ${\bm F}_t = \left( F_{1t}, \ldots, F_{K_F t} \right)^{\top}$ denotes the common factors
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	For some blocks, it may be appropriate to break up the data into subblocks, which adds another source of variation.
	Let $Z_{bsnt}$ be the $n$th series in subblock $s$ of block $b$.
	A four-level representation of the subblock data is given as
	\begin{align*}
		Z_{bsnt} &= {\bm \lambda}^n_{H.bs} \left( L \right) {\bm H}_{bst} + e_{Zbsnt} \\
		{\bm H}_{bst} &= {\bm \Lambda}_{G.bs} \left( L \right) {\bm G}_{bt} + {\bm e}_{Hbst} \\
		{\bm G}_{bt} &= {\bm \Lambda}_{F.b} \left( L \right) {\bm F}_t + {\bm e}_{Gbt} \\
		{\bm \Psi}_F \left( L \right) {\bm F}_t &= {\bm \epsilon}_{Ft}
	\end{align*}	
	where
	\begin{itemize}
		\item ${\bm \lambda}^n_{H.bs} \left( L \right)$ denotes a distributed lag polynomial of subblock-level factor loadings
		\item ${\bm \Lambda}_{G.bs} \left( L \right)$ denotes a distributed lag matrix polynomial of block-level factor loadings
		\item ${\bm H}_{bst} = \left( H_{bs1t}, \ldots, H_{bsK_{Hbs}t} \right)^{\top}$ denotes the subblock-level factors
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	The idiosyncratic components, the subblock-specific, block-specific, and common factors are assumed to be stationary, Gaussian autoregressive processes of orders $q_{Zbsn}$, $q_{Xbn}$, $q_{Hbsi}$, $q_{Gbj}$, and $q_{Fk}$, respectively, i.e.
	\begin{alignat*}{3}
		\psi_{Z.bsn} \left( L \right) e_{Zbsnt} &= \epsilon_{Zbsnt}, \quad & \epsilon_{Zbsnt} &\sim \mathcal{N} \left( 0, \sigma^2_{Zbsn} \right) \quad & n &= 1, \ldots, N_{bs} \\
		\psi_{X.bn} \left( L \right) e_{Xbnt} &= \epsilon_{Xbnt}, \quad & \epsilon_{Xbnt} &\sim \mathcal{N} \left( 0, \sigma^2_{Xbn} \right) \quad & n &= 1, \ldots, N_b \\
		\psi_{H.bsi} \left( L \right) e_{Hbsit} &= \epsilon_{Hbsit}, & \epsilon_{Hbsi} &\sim \mathcal{N} \left(0, \sigma^2_{Hbsi} \right) & i &= 1, \ldots, K_{Hbs} \\
		\psi_{G.bj} \left( L \right) e_{Gbjt} &= \epsilon_{Gbjt}, & \epsilon_{Gbjt} &\sim \mathcal{N} \left(0, \sigma^2_{Gbj} \right) & j &= 1, \ldots, K_{Gb} \\
		\psi_{F.k} \left( L \right) F_{kt} &= \epsilon_{Fkt}, & \epsilon_{Fkt} &\sim \mathcal{N} \left(0, \sigma^2_{Fk} \right) & k &= 1, \ldots, K_{F}
	\end{alignat*}
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	\begin{itemize}
		\item Not all series need to belong to blocks and subblocks, hence the data used in a four-level model are a mixture of $Z_{bsnt}$, $X_{bnt}$, and $X_{nt}$
		\item For brevity, we work with only the three-level model, therefore data will consist of $X_{bnt}$
	\end{itemize}	
\end{frame}

\begin{frame}
	\frametitle{Hierarchical Dynamic Factor Model}
	From \cite{moenchDynamicHierarchicalFactor2013}, this posterior distribution of the factors and model parameters is estimated via MCMC.
	The main steps are:
	\begin{enumerate}
		\item Organize data into blocks and subblocks.
		\item Get initial values for $\left\{ {\bm F}_t : t = 1, \ldots, T \right\}$ and $\left\{ {\bm G}_{bt} : b =  1, \ldots, B,  t = 1, \ldots, T \right\}$ via principal components
		\item Use the initial values for $\left\{ {\bm F}_t \right\}$ and $\left\{ {\bm G}_{bt} \right\}$ to obtain initial values for ${\bm \Lambda}$, ${\bm \Psi}$, and ${\bm \Sigma}$
		\item Conditional on ${\bm \Lambda}, {\bm \Psi}, {\bm \Sigma}, \left\{ {\bm F}_t \right\}$ and data draw $\left\{ {\bm G}_{bt} \right\}$
		\item Conditional on ${\bm \Lambda}, {\bm \Psi}, {\bm \Sigma}, \left\{ {\bm G}_{bt} \right\}$ and data draw $\left\{ {\bm F}_t \right\}$
		\item Conditional on $\left\{ {\bm F}_t \right\}, \left\{ {\bm G}_{bt} \right\}$ and data draw ${\bm \Lambda}, {\bm \Psi}, {\bm \Sigma}$
		\item Return to 3
	\end{enumerate}
\end{frame}

\section{Variational Bayesian Inference}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	Consider a model with data $\bm X$ and latent variables $\bm Z = \left\{ {\bm Z}_1, {\bm Z}_2 \right\}$.
  		The goal is to compute the joint posterior of the latent variables given the data
  		\begin{equation} \label{posterior}
  			p \left( {\bm Z} | {\bm X} \right) = \frac{ p \left( {\bm X} | {\bm Z} \right) p \left( {\bm Z} \right) }{ p \left( {\bm X} \right) }
  		\end{equation}
  		\begin{itemize}
  			\item Likelihood: $p \left( {\bm X} | {\bm Z} \right)$
			\item Prior: $p \left( {\bm Z}\right)$
			\item Evidence: $p \left( {\bm X} \right)$
			\begin{equation} 			
				p \left( {\bm X} \right) = \int p \left( {\bm X}, {\bm Z} \right) \, d{\bm Z} \label{marginal.evidence}
			\end{equation}
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	For complex models (\ref{posterior}) and (\ref{marginal.evidence}) typically have no closed-form.
	As a result, the joint and marginal posteriors have to be approximated.
	Markov chain Monte Carlo (MCMC) has been the gold standard to solve this problem.
	\begin{enumerate}
		\item Construct an ergodic Markov chain on $\bm Z$ whose stationary distribution is the joint posterior $p \left( {\bm Z} | {\bm X} \right)$
		\item Sample from the chain to collect samples from the stationary distribution
		\item Approximate the posterior with an empirical estimate constructed from a subset of the collected samples
		\item Use the subset of collected samples to estimate expectations of interest
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Why Variational Bayesian Inference?}
	A major disadvantage for MCMC is that high levels of accuracy requires a great deal of time
	\begin{enumerate}
		\item MCMC is typically slow, but ultimately accurate
		\item VB is typically much faster
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	Rather than use sampling, VB uses optimization to find an approximation to the posterior.
	\begin{enumerate}
		\item Posit a family of ``nice" approximate densities $\mathcal Q$
		\item Find a member of that family that is ``closest" to the exact posterior, i.e.
		\begin{equation}
			q^{\star} \left( {\bm Z} \right) = \argmin_{q \left( {\bm Z} \right) \in \mathcal Q} \text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) \label{objective}
		\end{equation}
		where 
		\begin{equation}
			\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \E_q \left[ \log q \left( {\bm Z} \right)\right] - \E_q \left[ \log p \left( {\bm Z} | {\bm X} \right) \right]
		\end{equation}
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	It follows that 
	\begin{equation*}
		\text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \E_q \left[ \log q \left( {\bm Z} \right)\right] - \E_q \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] + \log p \left( {\bm X} \right)
	\end{equation*}
	This reveals that the objective in (\ref{objective}) depends on the evidence, thus it cannot be computed directly.
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	Instead, we optimize an alternative objective that is equivalent to (\ref{objective}) up to an added constant called the \textbf{evidence lower bound} (ELBO)
	\begin{equation}
		\text{ELBO} \left( q \right) = \E_q \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] -\E_q \left[ \log q \left( {\bm Z} \right)\right] \label{ELBO}
	\end{equation}
	It can be shown that 
	\begin{equation*}
		\argmin_{q \left( {\bm Z} \right) \in \mathcal Q} \text{KL} \left( q \left( {\bm Z} \right) \, || \, p \left( {\bm Z} | {\bm X} \right) \right) = \argmax_{q \left( {\bm Z} \right) \in \mathcal Q} \text{ELBO} \left( q \right) 
	\end{equation*}
\end{frame}

\begin{frame}
	\frametitle{Variational Bayesian Inference}
	The VB framework is now
	\begin{enumerate}
		\item Posit a family of ``nice" approximate densities $\mathcal Q$
		\item Find a member of that family that is ``closest" to the exact posterior, i.e.
		\begin{equation}
			q^{\star} \left( {\bm Z} \right) = \argmax_{q \left( {\bm Z} \right) \in \mathcal Q} \text{ELBO} \left( q \right) \label{new.objective}
		\end{equation}
	\end{enumerate}
	Solving this optimization problem is still difficult in general
		\begin{itemize}
			\item Using the mean-field assumption can make it easier
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Mean-Field Assumption}
	\begin{enumerate}
		\item Partition the latent variables into $M$ groups, say ${\bm Z}_1, \ldots, {\bm Z}_M$
		\item Assume that the distributions in $\mathcal Q$ factorize across the groups, i.e.
		$$\mathcal Q = \left\{ q : q \left( {\bm Z} \right) = \prod_{m = 1}^M q_m \left( {\bm Z}_m \right) \right\}$$
		\item Learning the optimal $q$ now reduces to learning the optimal $q_1, \ldots, q_M$
		\item Straightforward to optimize via coordinate ascent
		\item This is \textbf{NOT} a modeling assumption
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Mean-Field Assumption}
	Interestingly, under the mean-field assumption, the optimization problem for  a single $q_m$ has the solution:
	\begin{equation}
		q_m \left( {\bm Z}_m \right) = \frac{ \exp \left\{ \E_{q_{-m}} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\} }{ \int \exp \left\{ \E_{q_{-m}} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\} \,d{\bm Z}_m }
	\end{equation}
	This establishes what is called the \textbf{coordinate ascent variational inference} algorithm.
\end{frame}

\begin{frame}
	\frametitle{Coordinate Ascent Variational Inference}
	\begin{algorithm}[H]
		\SetKwInput{Input}{Input}
		\SetKwInput{Output}{Output}
		\SetKwInput{Initialize}{Initialize}
		\SetAlgoLined
		\DontPrintSemicolon
		\Input{ Model $p \left( {\bm X}, {\bm Z} \right)$,  Data ${\bm X}$, }
		\Output{Variational density  $q \left( {\bm Z} \right) = \prod_{m=1}^M q_m \left( {\bm Z}_m \right)$}
		\Initialize{Variational densities $q_m \left( {\bm Z}_m \right)$}
		\While{the ELBO has not converged}{
			\For{$m \in \left\{ 1, 2, \ldots, M \right\}$}{
				Set  $q_m \left( {\bm Z}_m \right) \propto \exp \left\{ \E_{q_{-m}} \left[ \log p \left( {\bm X}, {\bm Z} \right) \right] \right\}$\;
			}
			Compute $\operatorname{ELBO} \left( q \right)$
	  	}
	  	\Return $q \left( {\bm Z} \right)$ \;
	 	\caption{Coordinate ascent variational inference}
	\end{algorithm}
\end{frame}

\subsection{Orthogonal Factor Model}
\begin{frame}
	\frametitle{The Orthogonal Factor Model}
	Here we present a VB framework for the orthogonal factor model.
\end{frame}

\begin{frame}
	\frametitle{The Orthogonal Factor Model}	
	The orthogonal FA model assumes the form 
	\begin{equation*}
		{\bm X} = {\bm \Lambda} {\bm F}+ {\bm e}
	\end{equation*}
	where
	\begin{enumerate}
		\item ${\bm X} = \left( X_{1}, \ldots, X_{N} \right)^{\top}$ denotes the vector of observations
		\item $\bm \Lambda = \left[ \lambda_{nk} \right]_{N \times K}$ denotes the matrix of factor loadings 
		\item ${\bm F} = \left( F_{1}, \ldots, F_{K} \right)^{\top}$ denotes the vector of latent factors
		\item ${\bm e} = \left( e_{1}, \ldots, e_{N} \right)^{\top}$ denotes the vector of latent error terms
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Assumptions of the Orthogonal Factor Model}
	The orthogonal FA model assumes the following:
	\begin{enumerate}
		\item $\bm F \sim \mathcal N \left( {\bm 0}_K, \bf{I}_K \right)$
		\item $\bm e \sim \mathcal N \left( {\bm 0}_N, \bm \Sigma \right)$ where $\bm \Sigma = \operatorname{diag} \left( \sigma_1^2, \ldots, \sigma_N^2 \right)$
		\item $F_k$ and $e_n$ are independent for every pair $k, n$
	\end{enumerate}
	where ${\bm 0}_K$ and ${\bm 0}_N$ are zero-vectors of lengths $K$ and $N$, respectively, and ${\bf I}_K$ is the $K \times K$ identity matrix.
\end{frame}

\section{Future Work}
\begin{frame}
	\frametitle{Future Work}
	The main goal of the project is to develop a variational Bayesian framework to handle the four-level model from \cite{moenchDynamicHierarchicalFactor2013}.
\end{frame}
\section{References}
\begin{frame}[allowframebreaks]
	\frametitle{References}
	\printbibliography
\end{frame}

\end{document}